{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading pretrained KoGPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTJModel(\n",
       "  (wte): Embedding(64512, 4096)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0): GPTJBlock(\n",
       "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): GPTJBlock(\n",
       "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): GPTJBlock(\n",
       "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): GPTJBlock(\n",
       "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): GPTJBlock(\n",
       "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): GPTJBlock(\n",
       "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): GPTJBlock(\n",
       "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): GPTJBlock(\n",
       "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): GPTJBlock(\n",
       "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): GPTJBlock(\n",
       "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): GPTJBlock(\n",
       "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): GPTJBlock(\n",
       "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (12): GPTJBlock(\n",
       "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (13): GPTJBlock(\n",
       "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (14): GPTJBlock(\n",
       "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (15): GPTJBlock(\n",
       "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (16): GPTJBlock(\n",
       "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (17): GPTJBlock(\n",
       "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (18): GPTJBlock(\n",
       "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (19): GPTJBlock(\n",
       "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (20): GPTJBlock(\n",
       "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (21): GPTJBlock(\n",
       "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (22): GPTJBlock(\n",
       "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (23): GPTJBlock(\n",
       "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (24): GPTJBlock(\n",
       "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (25): GPTJBlock(\n",
       "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (26): GPTJBlock(\n",
       "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (27): GPTJBlock(\n",
       "      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTJAttention(\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): GPTJMLP(\n",
       "        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)\n",
       "        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "  'kakaobrain/kogpt', revision='KoGPT6B-ryan1.5b-float16',  # or float32 version: revision=KoGPT6B-ryan1.5b\n",
    "  bos_token='[BOS]', eos_token='[EOS]', unk_token='[UNK]', pad_token='[PAD]', mask_token='[MASK]'\n",
    ")\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "  'kakaobrain/kogpt', revision='KoGPT6B-ryan1.5b-float16',  # or float32 version: revision=KoGPT6B-ryan1.5b\n",
    "  pad_token_id=tokenizer.eos_token_id,\n",
    "  torch_dtype='auto', low_cpu_mem_usage=True, output_hidden_states = True\n",
    ").to(device='cuda', non_blocking=True)\n",
    "\n",
    "_ = model.eval()\n",
    "_ = model.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(4096, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 256)\n",
    "        self.fc3 = nn.Linear(256, 64)\n",
    "        self.fc4 = nn.Linear(64, 12)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        x = torch.sigmoid(x) * 10\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sool_data.p', 'rb') as file: \n",
    "    sool_data = pickle.load(file)\n",
    "\n",
    "taste_list = []\n",
    "for i in sool_data:\n",
    "    taste_list.append([x for _, x in i['taste'].items()])\n",
    "taste_matrix = torch.tensor(taste_list).to(device).float()\n",
    "taste_matrix = (taste_matrix - taste_matrix.mean(dim=0)) / taste_matrix.std(dim=0) + 5\n",
    "\n",
    "data_list = []\n",
    "for idx, i in enumerate(sool_data):\n",
    "    if idx >= 100:\n",
    "        break\n",
    "    s = i['content'].split('\\n')\n",
    "    for j in s:\n",
    "        if j == '':\n",
    "            continue\n",
    "        ss = j.split('.')\n",
    "        for k in ss:\n",
    "            if k == '' or k == ' ':\n",
    "                continue\n",
    "            data_list.append( (idx, k) )\n",
    "\n",
    "validate_list = []\n",
    "for idx, i in enumerate(sool_data):\n",
    "    if idx < 100:\n",
    "        continue\n",
    "    s = i['content'].split('\\n')\n",
    "    for j in s:\n",
    "        if j == '':\n",
    "            continue\n",
    "        ss = j.split('.')\n",
    "        for k in ss:\n",
    "            if k == '' or k == ' ':\n",
    "                continue\n",
    "            validate_list.append( (idx, k) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preset = []\n",
    "for i in data_list:\n",
    "    t = tokenizer.encode(i[1], return_tensors='pt').to(device)\n",
    "    v = model(t).last_hidden_state[:, -1, :].float().view(-1, 1, 4096)\n",
    "    data_preset.append( (i[0], v) )\n",
    "\n",
    "val_preset = []\n",
    "for i in validate_list:\n",
    "    t = tokenizer.encode(i[1], return_tensors='pt').to(device)\n",
    "    v = model(t).last_hidden_state[:, -1, :].float().view(-1, 1, 4096)\n",
    "    val_preset.append( (i[0], v) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataSet(torch.utils.data.Dataset): \n",
    "  def __init__(self, data):\n",
    "      self.data = data\n",
    "  def __len__(self):\n",
    "      return len(self.data)\n",
    "  def __getitem__(self, idx):\n",
    "      x = self.data[idx][1].squeeze()\n",
    "      y = taste_matrix[self.data[idx][0]]\n",
    "      return x, y\n",
    "\n",
    "learnData = TextDataSet(data_preset)\n",
    "valData = TextDataSet(val_preset)\n",
    "\n",
    "\n",
    "learnLoader = DataLoader(learnData, batch_size = 50, shuffle = True)\n",
    "valLoader = DataLoader(valData, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th epoch starting.\n",
      "5th epoch starting.\n",
      "10th epoch starting.\n",
      "15th epoch starting.\n",
      "20th epoch starting.\n",
      "25th epoch starting.\n",
      "30th epoch starting.\n",
      "35th epoch starting.\n",
      "40th epoch starting.\n",
      "45th epoch starting.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = Classifier().to(device)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = optim.Adam(m.parameters(), lr=lr)\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "epochs = 50\n",
    "for epoch in range(epochs) :\n",
    "    m.train()\n",
    "    if(epoch % 5 == 0):\n",
    "        print(f\"{epoch}th epoch starting.\")\n",
    "\n",
    "    train_loss_lists = []\n",
    "    for x, y in learnLoader:\n",
    "        l = m(x)\n",
    "        optimizer.zero_grad()\n",
    "        train_loss = loss_function(l, y)\n",
    "        train_loss.backward()\n",
    "        train_loss_lists.append(train_loss.item())\n",
    "\n",
    "        optimizer.step()\n",
    "    train_loss_list.append(sum(train_loss_lists) / len(train_loss_lists))\n",
    "\n",
    "    m.eval()\n",
    "\n",
    "    val_loss_lists = []\n",
    "    for x, y in valLoader:\n",
    "        with torch.no_grad():\n",
    "            l = m(x)\n",
    "        val_loss = torch.abs(l - y).mean()\n",
    "\n",
    "        val_loss_lists.append(train_loss.item())\n",
    "    val_loss_list.append(sum(val_loss_lists) / len(val_loss_lists))\n",
    "\n",
    "plt.plot(list(range(epochs)), train_loss_list, 'r')\n",
    "plt.plot(list(range(epochs)), val_loss_list, 'g')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(m.state_dict(), 'full_model_test_2.p')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
